# **Let's write a Haiku! First fire up GPT-J. Then give it a prompt.**
## Generating rhythmic prose after finetuning a large transformer with phonemes

By Robert. A Gonsalves</br>

![image](https://raw.githubusercontent.com/robgon-art/DeepHaiku/main/deep_haiku.jpg)

You can see my article on Medium.

The source code and generated images are released under the [CC BY-SA license](https://creativecommons.org/licenses/by-sa/4.0/).</br>
![CC BYC-SA](https://licensebuttons.net/l/by-sa/3.0/88x31.png)

## Google Colabs
* [Deep Haiku Generator](https://colab.research.google.com/github/robgon-art/DeepHaiku/blob/main/Deep_Haiku_Generator.ipynb)

## Acknowledgements
- M. Grootendorst, KeyBERT: Minimal keyword extraction with BERT (2020)
- W. Zhu and S. Bhat, GRUEN for Evaluating Linguistic Quality of Generated Text (2020)
- M. Bernard, Phonemizer: Text to Phones Transcription for Multiple Languages in Python (2016)
- GPT-J, Mesh-Transformer-JAX: Model-Parallel Implementation of Transformer Language Model with JAX (2021)
- R. Caruana, Multitask learning (1997)
- E. Hu, et al., LoRA: Low-rank Adaptation of Large Language Models (2021)
- L. Hanu and the Unitary Team, Detoxify (2020)
